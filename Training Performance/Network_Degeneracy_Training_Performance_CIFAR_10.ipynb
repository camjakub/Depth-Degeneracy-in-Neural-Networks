{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Network Degeneracy as an Indicator of Training Performance\n",
        "### (CIFAR-10 Data)\n",
        "This notebook provides a summarized version of the code used to produce Figure 1 of *Network Degeneracy as an Indicator of Training Performance: Comparing Finite and Infinite Width Angle Predictions* (Jakub and Nica, 2023b). This notebook will use the CIFAR-10 dataset (Krizhevsky, 2009) smaller number of networks, and only 2 runs to allow it to run quickly on CoLab. The simulations ran in our paper used 45 different network architectures, ran 10 times each.\n",
        "\n",
        "You may run into issues with RAM if you try to run many runs of many architectures at once. Code is provided at the end of the notebook to save results, so you can run this notebook multiple times in smaller chunks and consolidate the results afterwards.\n",
        "\n",
        "Author: Cameron Jakub"
      ],
      "metadata": {
        "id": "EOPsVEpXCbN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras.models import Sequential # sequential is the feed forward NN\n",
        "from keras.layers.core import Dense, Dropout, Activation  # layer types\n",
        "from keras.utils import np_utils\n",
        "from matplotlib.pyplot import figure\n",
        "from google.colab import output\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "l7AI2LuzI_xA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Prepare CIFAR-10 Data"
      ],
      "metadata": {
        "id": "DfGuF2ZfJU-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the CIFAR-10 dataset\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# split into test and train, and X and Y variables\n",
        "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "Q_39gDdqJXTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f07355-648c-4506-9bc2-5e0eec810314"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot an image"
      ],
      "metadata": {
        "id": "3oXq0-KqLFnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change the value of img to look at different images\n",
        "img = 0\n",
        "plt.imshow(train_X[img,:,:]);"
      ],
      "metadata": {
        "id": "_5NYlk_8LHwM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "4ceef176-3b89-4d1b-ed5a-2b8db6636b08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before fitting our data to a neural network, we first reshape the data to be a vector, and then then normalize."
      ],
      "metadata": {
        "id": "urgHvu8_LVFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape\n",
        "train_X = train_X.reshape(50000, 3072)\n",
        "test_X = test_X.reshape(10000, 3072)\n",
        "\n",
        "# Convert from integer to float\n",
        "train_X = train_X.astype('float32')\n",
        "\n",
        "# Normalize\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255"
      ],
      "metadata": {
        "id": "wCWHkJEjLaB5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we convert the y vectors (target) to a matrix of classes, with a 1 in the appropriate class."
      ],
      "metadata": {
        "id": "Z46jGCgZLlxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = np_utils.to_categorical(train_Y, 10)\n",
        "test_y = np_utils.to_categorical(test_Y, 10)\n",
        "test_y"
      ],
      "metadata": {
        "id": "-JS7uZR7LmgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904c7a31-cde7-4a64-dcb4-e2aa8a2e3541"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Networks\n",
        "We want to run each model multiple times, so we can create a confidence interval for the performance of each network. Here, we can specify number of times we want each model to run."
      ],
      "metadata": {
        "id": "TQJHOA3x_wKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_runs = 2"
      ],
      "metadata": {
        "id": "mnfllkbc_yxy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify hyperparameters."
      ],
      "metadata": {
        "id": "TAlKRb8SOV3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size\n",
        "N_batch = 100\n",
        "\n",
        "# epochs\n",
        "N_epoch = 1"
      ],
      "metadata": {
        "id": "j8NVBtOVOXyZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are studying feed-forward ReLU networks with depth $L$ and layer widths $1\\leq \\ell \\leq L$. First, we can formally define a network $f_L(x): \\mathbb{R}^{n_{in}} \\to \\mathbb{R}^{n_{out}}$. Given some input $x \\in \\mathbb{R}^{n_{in}}$, the first hidden layer of our network $z^1$ can be calculated as\n",
        "\n",
        "$$ z^1 = W^1x,$$\n",
        "\n",
        "where $W^1 \\in \\mathbb{R}^{n_1 \\times n_{in}}$ is a matrix of weights. To get from layer $\\ell$ to layer $\\ell+1$, we take the current layer and\n",
        "* apply the ReLU activation function $\\varphi(x)$ entry-wise\n",
        "* multiply by weight matrix $W^{\\ell+1}$\n",
        "* apply a normalization constant of $\\sqrt{2/n_\\ell}$\n",
        "\n",
        "$$ z^{\\ell+1} = \\sqrt{\\frac{2}{n_\\ell}}W^{\\ell+1} \\varphi(z^\\ell) $$\n",
        "\n",
        "Finally, the output of our network is simply the final layer of the network.\n",
        "\n",
        "$$ f_L(x) = z^L.$$\n",
        "\n",
        "### Details on the Normalization Constant\n",
        "\n",
        "The normalization constant we use is known as the \"He\" normalization constant (He et al., 2015). To account for this in our code, we specify the kernel initializer to be 'HeNormal\" for each layer. You may notice that this normalization constant does not appear in our calculation of the first hidden layer, $z_1$. It is common in practice to skip the normalization in the first layer, but if we don't normalize each subsequent layer, it can cause the nodes of our network to become extremely large. This is because each node of layer $\\ell+1$ is calculated as the inner product between the current layer $z^\\ell$ and the corresponding row of the weight matrix $W^{\\ell+1}$. Each inner product is a **sum** of $n_\\ell$ numbers, so without normalization, these numbers would grow unbounded layer by layer. Including the normalization constant turns the sums over $n_\\ell$ numbers into an **average** over $n_\\ell$ numbers, preventing them from \"blowing up\".\n",
        "\n",
        "For a classification problem like this one, whether we include the normalization constant or not in the first layer is equivalent to changing the temperature of the softmax function, which will not actually change how the networks classify the digits. The softmax function is defined as\n",
        "\n",
        "$$\\textrm{softmax}(z)_i = \\frac{e^{\\frac{z_i}{T}}}{\\sum_{j=1}^K e^{\\frac{z_j}{T}}} ,$$\n",
        "where $z$ is the input vector, $K$ is the number of classes, and $T$ is the temperature of the function.\n",
        "\n",
        "To be more consistent with our update rule, which was developed to predict how the inputs evolve from one *hidden layer*  to the next, we will include the normalization in the first layer - ie. $z_1 = \\sqrt{2/n_\\ell}W^1 x$, knowing that this will not actually affect the test accuracy of each network.\n",
        "\n",
        "\n",
        "We want to compare the training performance of multiple networks with various architectures. To do this easily, we create a function that will create our model based on a vector of ordered layer widths given to it."
      ],
      "metadata": {
        "id": "3eeNDQJFgu6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function which returns your model based on the architecture its fed\n",
        "def create_model(layers):\n",
        "  depth = len(layers)\n",
        "  mdl = Sequential() # sequential represents a feed-forward nnet\n",
        "\n",
        "  # add first hidden layer\n",
        "  mdl.add(Dense(layers[0], input_shape=(3072,), use_bias = False, kernel_initializer = 'HeNormal'))\n",
        "\n",
        "  # add activation function\n",
        "  mdl.add(Activation('relu'))\n",
        "\n",
        "  # add rest of hidden layers\n",
        "  for i in range(1, depth):\n",
        "    mdl.add(Dense(layers[i], use_bias = False, kernel_initializer = 'HeNormal'))\n",
        "    mdl.add(Activation('relu'))\n",
        "\n",
        "  # add output layer\n",
        "  mdl.add(Dense(10, use_bias = False, kernel_initializer = 'HeNormal'))\n",
        "\n",
        "  # convert output vector to a probability vector w/ softmax\n",
        "  mdl.add(Activation('softmax'))\n",
        "\n",
        "  return mdl"
      ],
      "metadata": {
        "id": "reHHyG_gb_xD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a function which takes a model, compiles and trains the model, and outputs the model's test accuracy."
      ],
      "metadata": {
        "id": "2DYBWYgmhou5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_score(mdl):\n",
        "  # compile the model\n",
        "  mdl.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # train the model\n",
        "  mdl.fit(train_X, train_y, batch_size=N_batch, epochs=N_epoch, verbose=1)\n",
        "\n",
        "  # extract the test accuracy of the model\n",
        "  score = mdl.evaluate(test_X, test_y)[1]\n",
        "\n",
        "  return score"
      ],
      "metadata": {
        "id": "ryeHdSW3j_qH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to create a bunch of model architectures, which we will define as an ordered list of hidden layer widths."
      ],
      "metadata": {
        "id": "jpNbotupm5IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_widths = []\n",
        "\n",
        "layer_widths.append(np.tile(85,2).tolist())\n",
        "layer_widths.append(np.tile(100, 10).tolist())\n",
        "layer_widths.append(np.tile(30, 15).tolist())\n",
        "layer_widths.append(np.tile(15,30).tolist())\n",
        "layer_widths.append(np.tile(75,35).tolist())\n",
        "layer_widths.append(np.tile(50, 50).tolist())"
      ],
      "metadata": {
        "id": "rAzVPyY7m4tX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create and train each model. For the purpose of keeping track of observations, we will also assign each model a number and keep track of the order in which each model ran. To create a plot, we also want to save the number of parameters in each model, as well as the \"score\" of each model, representing its classification accuracy on the test data.\n",
        "\n",
        "Note: the below cell may take many minutes to run, as this is where we create, train, and evaluate each of the models created."
      ],
      "metadata": {
        "id": "QjugmhmrpLV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [create_model(l) for l in layer_widths*N_runs]\n",
        "model_num = [i for i,x in list(enumerate(layer_widths))*N_runs]\n",
        "\n",
        "params = [m.count_params() for m in models]\n",
        "scores = [model_score(m) for m in models]"
      ],
      "metadata": {
        "id": "G68Ou8A6pKlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d47c215-008d-4251-9a57-748e51e59a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 [==============================] - 7s 3ms/step - loss: 1.9183 - accuracy: 0.3099\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.8158 - accuracy: 0.3382\n",
            "500/500 [==============================] - 4s 5ms/step - loss: 1.9741 - accuracy: 0.2685\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.8150 - accuracy: 0.3326\n",
            "500/500 [==============================] - 5s 4ms/step - loss: 2.0787 - accuracy: 0.1986\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.9489 - accuracy: 0.2457\n",
            "500/500 [==============================] - 9s 6ms/step - loss: 2.2486 - accuracy: 0.1351\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.1915 - accuracy: 0.1599\n",
            "500/500 [==============================] - 9s 7ms/step - loss: 2.1579 - accuracy: 0.1529\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.0729 - accuracy: 0.1698\n",
            "500/500 [==============================] - 14s 10ms/step - loss: 2.2046 - accuracy: 0.1135\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 2.0977 - accuracy: 0.1684\n",
            "500/500 [==============================] - 2s 3ms/step - loss: 1.9061 - accuracy: 0.3129\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.7644 - accuracy: 0.3725\n",
            "500/500 [==============================] - 5s 5ms/step - loss: 1.9722 - accuracy: 0.2693\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.8611 - accuracy: 0.3276\n",
            "500/500 [==============================] - 5s 4ms/step - loss: 2.0977 - accuracy: 0.1856\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 1.9658 - accuracy: 0.2544\n",
            "500/500 [==============================] - 8s 6ms/step - loss: 2.2102 - accuracy: 0.1400\n",
            "313/313 [==============================] - 2s 4ms/step - loss: 2.0985 - accuracy: 0.1556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict the Angle at Layer $L$\n",
        "\n",
        "At the core of our predictions is the function $\\mu(\\theta,n)$, defined in Theorem 1 of Jakub and Nica (2023a). Given the angle between inputs at the current layer ($\\theta^\\ell$), the function $\\mu(\\theta_\\ell, n_\\ell)$ is used to approximate $\\ln(\\sin^2(\\theta^{\\ell+1}))$:\n",
        "\n",
        "$$\\mathbf{E}[\\ln(\\sin^2(\\theta^{\\ell+1}))] = \\mu(\\theta^\\ell, n_\\ell) + \\mathcal{O}(n_\\ell^{-2}).$$\n",
        "\n",
        "We define $\\mu$ in terms of a family of $J$ functions which represent the joint moments of the ReLU activation function applied to Gaussian variables. For more on the $J$ functions, see Section 3 of Jakub and Nica (2023a).\n",
        "\n",
        "Here, we define the necessary $J$ functions ($J_{1,1}(\\theta), J_{2,2}(\\theta), J_{3,1}(\\theta)$)."
      ],
      "metadata": {
        "id": "-GzzFgC4aLzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j11 = lambda th: (np.sin(th) + (np.pi - th)*np.cos(th))/(2*np.pi)\n",
        "\n",
        "j22 = lambda th: ((np.pi-th)*(2*np.cos(th)**2 + 1) + 3*np.sin(th)*np.cos(th))/(2*np.pi)\n",
        "\n",
        "j31 = lambda th: (3*(np.pi-th)*np.cos(th) + np.sin(th)*np.cos(th)**2 + 2*np.sin(th))/(2*np.pi)"
      ],
      "metadata": {
        "id": "1tJk0twcayfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define our formula for $\\mu(\\theta, n)$ in terms of the $J$ functions:\n",
        "\n",
        "$$\\mu(\\theta,n) := \\ln \\left( \\frac{(n-1)(1-4J_{1,1}^2)}{4J_{2,2}-1+n}\\right)+ \\frac{ 4(J_{2,2}+1)}{n\\left(\\frac{4J_{2,2}-1}{n}+1\\right)^2}-  \\frac{4\\left(8J_{1,1}^2J_{2,2} - 8J_{1,1}^4 + 4J_{1,1}^2 - 8J_{1,1}J_{3,1} + J_{2,2} + 1 \\right) }{n\\left( 1-\\frac{1}{n} \\right)^2 \\left( 1 - 4 J_{1,1}^2 \\right)^2 }. $$\n",
        "\n"
      ],
      "metadata": {
        "id": "Gt-Whdvta1Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mu(th, n):\n",
        "  return np.log((n-1)*(1-4*j11(th)**2)/(4*j22(th)-1+n)) \\\n",
        "         + 4*(j22(th)+1)/(n*((4*j22(th)-1)/n + 1)**2) \\\n",
        "         - 4*(8*j11(th)**2*j22(th) - 8*j11(th)**4 + 4*j11(th)**2 - 8*j11(th)*j31(th) + j22(th) +1) \\\n",
        "         /((n-1)**2*(1-4*j11(th)**2)**2/n)"
      ],
      "metadata": {
        "id": "67ulIA4laSWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have $\\mu$ properly defined, which is at the heart of our prediction algorithm. Using the width of each layer, we can predict the angle between 2 orthogonal inputs at the final layer of each network.\n",
        "\n",
        "$$ \\ln(\\sin^2(\\theta^{\\ell+1})) = \\mu(\\theta^\\ell, n_\\ell).$$\n"
      ],
      "metadata": {
        "id": "yZUkLHeZeSvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def theta_L(width_vector):\n",
        "  net_depth = np.shape(width_vector)[0]-1\n",
        "\n",
        "  # Create vector to store theta values at each layer\n",
        "  theta = np.zeros((net_depth+1,1))\n",
        "\n",
        "  # Create the starting angle, representing orthogonal inputs\n",
        "  theta[0] = np.pi/2\n",
        "\n",
        "  # loop to predict theta at layer L\n",
        "  for i in range(net_depth):\n",
        "    # predict E[ln(sin^2(th))] at the next layer\n",
        "    ln_sinsq_th = mu(theta[i], width_vector[i])\n",
        "\n",
        "    # convert ln(sin^2(th)) back to th\n",
        "    theta[i+1] = np.arcsin(np.exp(ln_sinsq_th/2))\n",
        "\n",
        "  return theta[net_depth][0]"
      ],
      "metadata": {
        "id": "wcoVnEWuSpU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the full architecture of the network, we will add the dimension of the input and output layer to our vector of hidden layer widths."
      ],
      "metadata": {
        "id": "96-RVazFxgS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_in = 3072\n",
        "N_out = 10\n",
        "\n",
        "for l in layer_widths:\n",
        "  l.insert(0,N_in)\n",
        "  l.append(N_out)"
      ],
      "metadata": {
        "id": "igd_JkDzwnZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to compute the final angle between inputs for each trained network:"
      ],
      "metadata": {
        "id": "sXUnJZ9D1qz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thetas = [theta_L(l) for l in layer_widths*N_runs]"
      ],
      "metadata": {
        "id": "Y7DYH2QJ1o84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting\n",
        "We want to create error bars for each network used. To do this, we group together observations which came from the same network architectures.\n"
      ],
      "metadata": {
        "id": "KhWIl0-Wm0dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(params, thetas, model_num, scores):\n",
        "  # for repeated runs of the same architecture, the final predicted angle and\n",
        "  # the number of parameters will not change. Therefore, for each architecure,\n",
        "  # we only need to store this value once\n",
        "  thetas_plot = [thetas[i] for i in set(model_num)]\n",
        "  params_plot = [params[i] for i in set(model_num)]\n",
        "\n",
        "  # For each architecture, we want to use the multiple runs to create an error\n",
        "  # bar for test performance. Here, we use the multiple runs to calculate the\n",
        "  # mean and error margin for a 95% confidence interval\n",
        "  means = []\n",
        "  err_margins = []\n",
        "\n",
        "  for n in set(model_num):\n",
        "    # find the indices of the current model\n",
        "    idx = [i for i,j in enumerate(model_num) if j==n]\n",
        "\n",
        "    # create an error bar for the scores corresponding to current theta value\n",
        "    theta_scores = [scores[i] for i in idx]\n",
        "\n",
        "    # calculate the mean\n",
        "    means.append(np.mean(theta_scores))\n",
        "\n",
        "    # calculate standard error\n",
        "    std_error = np.std(theta_scores)/np.sqrt(10)\n",
        "    err_margins.append(std_error*1.96)\n",
        "\n",
        "  return params_plot, thetas_plot, means, err_margins"
      ],
      "metadata": {
        "id": "xgMLiWA37qyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will express the colorbar in terms of $\\log_{10}(\\# \\textrm{ of parameters})$, and we will plot the $\\theta$ values on the $x$ axis as $\\ln(\\sin^2(\\theta))$ to be consistent with $\\mu(\\theta,n)$, which we used to predict $\\mathbf{E}[\\ln(\\sin^2(\\theta))]$."
      ],
      "metadata": {
        "id": "Vtp1IWDIEwja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_plot, thetas_plot, means, err_margins = plot_data(params, thetas, model_num, scores)\n",
        "\n",
        "log_params = [math.log10(p) for p in params_plot]\n",
        "log_thetas = [np.log(np.sin(th)**2) for th in thetas_plot]"
      ],
      "metadata": {
        "id": "B-Z5H3i2FTW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we are ready to plot the results."
      ],
      "metadata": {
        "id": "Kqh8mTE3BrA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(6, 4), dpi = 150)\n",
        "\n",
        "plt.scatter(log_thetas, means, c = log_params, cmap = 'viridis', marker = '.')\n",
        "plt.xlim([-14,0])\n",
        "plt.xlabel(\"$\\\\ln(\\\\sin^2(\\\\theta^L))$\", fontsize = 14)\n",
        "plt.ylabel(\"Test Accuracy\", fontsize = 14)\n",
        "plt.title('Finite Width Prediction', fontsize = 16)\n",
        "\n",
        "# add in colourbar\n",
        "cbar = plt.colorbar(pad=0.01)\n",
        "\n",
        "# add in coloured error bars\n",
        "cmap = plt.cm.viridis\n",
        "norm = matplotlib.colors.Normalize()\n",
        "plt.errorbar(log_thetas, means, yerr=err_margins, ls='none', ecolor = cmap(norm(log_params)))"
      ],
      "metadata": {
        "id": "IheUv2H9WPwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Save Plotting Data to Google Drive\n",
        "If you wish to run and plot many network architectures at once, it is likely you will run out of available RAM on CoLab. The below code can be used to save your results to a file on your Google Drive. You can then change the network architectures and re-run the notebook, and the results will be appended to the files created on the first run."
      ],
      "metadata": {
        "id": "GM9GrJQUAYVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive to save files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "9lhOff6tOuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we make 2 functions which allow us to retrieve data from old runs, and update files on our Google Drive to include the data from new runs."
      ],
      "metadata": {
        "id": "0K5IDc0Xv0hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_file(path):\n",
        "  with open(path, 'rb') as fp:\n",
        "    return pickle.load(fp)\n",
        "\n",
        "def update_file(path, data):\n",
        "  with open(path, 'wb') as fp:\n",
        "    pickle.dump(data, fp)"
      ],
      "metadata": {
        "id": "ao3RgxwcvHj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specify path which data will be saved in your Google Drive\n",
        "mypath = Path('/content/drive/My Drive/insert_path') # update this line with your desired path\n",
        "\n",
        "# create the path names for each of the files you will be saving\n",
        "params_path = Path.joinpath(mypath, 'params_file')\n",
        "thetas_path = Path.joinpath(mypath, 'thetas_file')\n",
        "means_path = Path.joinpath(mypath, 'mean_score_file')\n",
        "err_margins_path = Path.joinpath(mypath, 'err_margins_file')\n",
        "\n",
        "# check if these files have been saved before\n",
        "if os.path.exists(params_path):\n",
        "\n",
        "  # open files from previous runs\n",
        "  params_old = retrieve_file(params_path)\n",
        "  thetas_old = retrieve_file(thetas_path)\n",
        "  means_old = retrieve_file(means_path)\n",
        "  err_margins_old = retrieve_file(err_margins_path)\n",
        "\n",
        "  # Extend the current lists to include the data from past runs\n",
        "  params_plot.extend(params_old)\n",
        "  thetas_plot.extend(thetas_old)\n",
        "  means.extend(means_old)\n",
        "  err_margins.extend(err_margins_old)"
      ],
      "metadata": {
        "id": "2vI_NIc2PRyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our variables include all the data from the current run, plus the data from previous runs, we can update the files on our drive."
      ],
      "metadata": {
        "id": "xEYt22wePTub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_old = update_file(params_path, params_plot)\n",
        "thetas_old = update_file(thetas_path, thetas_plot)\n",
        "means_old = update_file(means_path, means)\n",
        "err_margins_old = update_file(err_margins_path, err_margins)"
      ],
      "metadata": {
        "id": "a-zt2sLQPWPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into recti-\n",
        "fiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE\n",
        "International Conference on Computer Vision (ICCV), pages 1026–1034, 2015. doi:\n",
        "10.1109/ICCV.2015.123\n",
        "\n",
        "Cameron Jakub and Mihai Nica. Depth degeneracy in neural networks: Vanishing angles in\n",
        "fully connected ReLU networks on initialization, 2023a. URL https://arxiv.org/abs/2302.09712\n",
        "\n",
        "Cameron Jakub and Mihai Nica. Network degeneracy as an indicator of training performance: Comparing finite and infinite width angle predictions, 2023b. URL https://arxiv.org/abs/2306.01513\n",
        "\n",
        "Alex Krizhevsky. Learning multiple layers of features from tiny images. pages 32–33, 2009. URL https://www.cs.toronto.edu/ ̃kriz/learning-features-2009-TR.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "lXu5Amgx_N_0"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}